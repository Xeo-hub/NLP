{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# <font color='#2B4865'>**Transformer's Attention Mechanism**</font>\n",
        "\n",
        "---\n",
        "### Natural Language Processing\n",
        "Date: Dec 13, 2022\n",
        "\n",
        "Last Update: Nov 23, 2023\n",
        "\n",
        "---\n",
        "\n",
        "This notebook is based on the reference notebooks from [Denis Rothman](https://github.com/Denis2054/Transformers-for-NLP-2nd-Edition/blob/main/Chapter02/Multi_Head_Attention_Sub_Layer.ipynb) and [Manuel Romero.](https://colab.research.google.com/drive/1rPk3ohrmVclqhH7uQ7qys4oznDdAhpzF)\n",
        "\n",
        "Our goal here is to obtain a mathematical view of the Transformer's Multi-Head attention mechanism."
      ],
      "metadata": {
        "id": "hwjTVnY1mBsn"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veRoFjFRNXwJ"
      },
      "source": [
        "%pip install --quiet colored\n",
        "\n",
        "from colored import Fore, Back, Style\n",
        "import numpy as np\n",
        "from scipy.special import softmax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color='#2B4865'>**Step 1: Represent the input**\n",
        "---\n",
        "</font>"
      ],
      "metadata": {
        "id": "BDgUg7ZVm4mp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For visualization purposes, we are scaling down the input of the attention mechanism from $d_{model}=512$ as in the original Transformer model to $d_{model}=4$. This brings the dimensions of the vector of an input $x$ to the Transformer model down to $d_{model}=4$.\n",
        "\n",
        "<br><center><img src=\"https://drive.google.com/uc?id=1uGolVso4Z72KA88aQ3wWCoS-k9Vd8Y70\" width=\"60%\"></center><br>"
      ],
      "metadata": {
        "id": "XtP-nuIUva93"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLe9lWCJNogW",
        "outputId": "bc122ac1-0bfa-4981-c90d-131d000f49b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "x =np.array([[1.0, 0.0, 1.0, 0.0],   # Input #1\n",
        "             [0.0, 2.0, 0.0, 2.0],   # Input #2\n",
        "             [1.0, 1.0, 1.0, 1.0]])  # Input #3\n",
        "print(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1. 0. 1. 0.]\n",
            " [0. 2. 0. 2.]\n",
            " [1. 1. 1. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color='#2B4865'>**Step 2: Initialize weight matrices**\n",
        "---\n",
        "</font>"
      ],
      "metadata": {
        "id": "Kx_oiVbvyXKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Every input must have three representations, namely <font color='#80D0FF'>queries</font>, <font color='#82D9B3'>keys</font> and <font color='#F89797'>values</font>. To obtain these representations, **each input has three weight matrices:**\n",
        "\n",
        "*   <font color='#80D0FF'>$W^Q$</font> to train the <font color='#80D0FF'>queries</font>\n",
        "*   <font color='#82D9B3'>$W^K$</font> to train the <font color='#82D9B3'>keys</font>\n",
        "*   <font color='#F89797'>$W^V$</font> to train the <font color='#F89797'>values</font>\n",
        "\n",
        "In the original Transformer model, these matrices are of $d_{k}=64$ dimensions, but here we will be scaling them down to $d_{k}=3$. Because every input has a dimension of $4$, this means each set of the weights must have a shape of $4\\times 3$.\n",
        "\n",
        "<br><center><img src=\"https://drive.google.com/uc?id=1CFA4GNYXHlGIF95hw9jjDvj_vpWYMuOn\" width=\"60%\"></center><br>\n",
        "\n",
        "*Note: In a neural network setting, these weights are usually small numbers, initialized randomly using an appropriate random distribution like Gaussian, Xavier, and Kaiming distributions.*"
      ],
      "metadata": {
        "id": "VYg4M5Opyisf"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZImwtHPN91V",
        "outputId": "2095998d-1142-464d-ac5c-635db7897365",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "w_query =np.array([[1, 0, 1],\n",
        "                   [1, 0, 0],\n",
        "                   [0, 0, 1],\n",
        "                   [0, 1, 1]])\n",
        "\n",
        "w_key =np.array([[0, 0, 1],\n",
        "                 [1, 1, 0],\n",
        "                 [0, 1, 0],\n",
        "                 [1, 1, 0]])\n",
        "\n",
        "w_value = np.array([[0, 2, 0],\n",
        "                    [0, 3, 0],\n",
        "                    [1, 0, 3],\n",
        "                    [1, 1, 0]])\n",
        "\n",
        "print(Fore.LIGHT_BLUE + Style.BOLD + \"Weights for query: \\n\" + Style.RESET, w_query)\n",
        "print(Fore.LIGHT_GREEN + Style.BOLD + \"Weights for key: \\n\" + Style.RESET, w_key)\n",
        "print(Fore.LIGHT_RED + Style.BOLD + \"Weights for value: \\n\" + Style.RESET, w_value)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;12m\u001b[1mWeights for query: \n",
            "\u001b[0m [[1 0 1]\n",
            " [1 0 0]\n",
            " [0 0 1]\n",
            " [0 1 1]]\n",
            "\u001b[38;5;10m\u001b[1mWeights for key: \n",
            "\u001b[0m [[0 0 1]\n",
            " [1 1 0]\n",
            " [0 1 0]\n",
            " [1 1 0]]\n",
            "\u001b[38;5;9m\u001b[1mWeights for value: \n",
            "\u001b[0m [[0 2 0]\n",
            " [0 3 0]\n",
            " [1 0 3]\n",
            " [1 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color='#2B4865'>**Step 3: Matrix multiplication to obtain queries, keys and values**\n",
        "---\n",
        "</font>"
      ],
      "metadata": {
        "id": "wLFwmGQhDO1b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have the three sets of weights, we obtain the <font color='#80D0FF'>query</font>, <font color='#82D9B3'>key</font> and <font color='#F89797'>value</font> representations for every input. We do this by multiplying the input vectors by the weight matrices:\n",
        "\n",
        "<br><center><img src=\"https://drive.google.com/uc?id=10UTnlUdEf9xtbaCrcJqrjg9A24NUPwiM\" width=\"60%\"></center><br>\n",
        "\n",
        "*Note: In practice, a bias vector may be added to the product of matrix multiplication*"
      ],
      "metadata": {
        "id": "y0mI9ecRDYG7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Q=np.matmul(x,w_query)\n",
        "K=np.matmul(x,w_key)\n",
        "V=np.matmul(x,w_value)\n",
        "\n",
        "print(Fore.LIGHT_BLUE + Style.BOLD + \"Queries: \\n\" + Style.RESET, Q)\n",
        "print(Fore.LIGHT_GREEN + Style.BOLD + \"Keys: \\n\" + Style.RESET, K)\n",
        "print(Fore.LIGHT_RED + Style.BOLD + \"Values: \\n\" + Style.RESET, V)"
      ],
      "metadata": {
        "id": "_De-dE9dEols",
        "outputId": "eebd8cf1-a3cd-4ccc-93d0-52a5d7399041",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;12m\u001b[1mQueries: \n",
            "\u001b[0m [[1. 0. 2.]\n",
            " [2. 2. 2.]\n",
            " [2. 1. 3.]]\n",
            "\u001b[38;5;10m\u001b[1mKeys: \n",
            "\u001b[0m [[0. 1. 1.]\n",
            " [4. 4. 0.]\n",
            " [2. 3. 1.]]\n",
            "\u001b[38;5;9m\u001b[1mValues: \n",
            "\u001b[0m [[1. 2. 3.]\n",
            " [2. 8. 0.]\n",
            " [2. 6. 3.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color='#2B4865'>**Step 4: Calculate scaled attention scores**\n",
        "---\n",
        "</font>"
      ],
      "metadata": {
        "id": "IGAWOXu-FPN4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The attention head now implements the original Transformer equation:\n",
        "\n",
        "$$Attention(\\mathbf{Q,K,V}) = softmax\\left(\\frac{\\mathbf{QK^T}}{\\sqrt{d_k}}\\right)\\mathbf{V}$$\n",
        "\n",
        "This step focuses on $\\mathbf{Q}$ and $\\mathbf{K}$:\n",
        "\n",
        "$$\\left(\\frac{\\mathbf{QK^T}}{\\sqrt{d_k}}\\right)$$\n",
        "\n",
        "We start by calculating the **scaled attention scores** for Input #1 by taking a scaled dot product between Input #1's <font color='#80D0FF'>query</font> with **all** <font color='#82D9B3'>keys</font>, including itself:\n",
        "\n",
        "$$\\text{Score of Input #1} = Q_1 \\cdot K \\hspace{2mm} \\text{(all three keys)}$$\n",
        "\n",
        "Since there are 3 key representations (because we have 3 inputs), we obtain 3 attention scores for Input #1:\n",
        "\n",
        "$$\\text{Score of Input #1} = [2, 4, 4]$$\n",
        "\n",
        "Then, we repeat the same steps for both Input #2 and Input #3. In practice, we calculate the attention scores for all three inputs at once in matrix form.\n",
        "\n",
        "*Note: For this example, we will round $\\sqrt{d_k}=\\sqrt{3}=1.75$ to $1$ to simplify the computations*"
      ],
      "metadata": {
        "id": "JLer9WrLFWhi"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfgRAHUuOp5c",
        "outputId": "8509c619-382f-49c0-a451-d94ce7ae5d31",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "k_d=1\n",
        "attention_scores = (Q @ K.transpose())/k_d\n",
        "print(Fore.YELLOW + Style.BOLD + \"Attention scores: \\n\" + Style.RESET, attention_scores)\n",
        "\n",
        "# tensor([[ 2.,  4.,  4.],  # attention scores from Query 1\n",
        "#         [ 4., 16., 12.],  # attention scores from Query 2\n",
        "#         [ 4., 12., 10.]]) # attention scores from Query 3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;3m\u001b[1mAttention scores: \n",
            "\u001b[0m [[ 2.  4.  4.]\n",
            " [ 4. 16. 12.]\n",
            " [ 4. 12. 10.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br><center><img src=\"https://drive.google.com/uc?id=1vGdAIEDVa1eg3XbdXupHJcveKWsOEpmU\" width=\"60%\"></center><br>\n"
      ],
      "metadata": {
        "id": "vds-If3pcANe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color='#2B4865'>**Step 5: Scaled softmax attention scores**\n",
        "---\n",
        "</font>"
      ],
      "metadata": {
        "id": "qmkT-YnSLIrE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now apply a softmax function to each intermediate attention score. Instead of doing a matrix multiplication, let's zoom down to each vector:"
      ],
      "metadata": {
        "id": "dn2jdP5F9izR"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hg2t6KuNOjzM",
        "outputId": "7c01a4c5-3985-4813-8135-56c9be2d0fab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "attention_scores[0]=softmax(attention_scores[0])\n",
        "attention_scores[1]=softmax(attention_scores[1])\n",
        "attention_scores[2]=softmax(attention_scores[2])\n",
        "print(Fore.YELLOW + Style.BOLD + \"Scaled softmax attention scores: \" + Style.RESET)\n",
        "print(attention_scores[0])\n",
        "print(attention_scores[1])\n",
        "print(attention_scores[2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;3m\u001b[1mScaled softmax attention scores: \u001b[0m\n",
            "[0.06337894 0.46831053 0.46831053]\n",
            "[6.03366485e-06 9.82007865e-01 1.79861014e-02]\n",
            "[2.95387223e-04 8.80536902e-01 1.19167711e-01]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With this, we obtain a scaled softmax attention score for each vector. For example, the softmax of the score of $x_1$ is:\n",
        "\n",
        "$$\\text{Softmax(Score of Input #1)} = [0.06, 0.46, 0.46]$$"
      ],
      "metadata": {
        "id": "VqiPNxMPdhQJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br><center><img src=\"https://drive.google.com/uc?id=1KU1KdQPkFzdS94e3NoVo1w9TaJC6Jz0m\" width=\"60%\"></center><br>"
      ],
      "metadata": {
        "id": "VcXIk43Xdi_V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color='#2B4865'>**Step 6: The final attention representations**\n",
        "---\n",
        "</font>"
      ],
      "metadata": {
        "id": "j3gNJL20-mEr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the results obtained in Step 5, we can now finalize the complete attention equation presented in Step 4 by plugging V in:\n",
        "\n",
        "$$Attention(\\mathbf{Q,K,V}) = softmax\\left(\\frac{\\mathbf{QK^T}}{\\sqrt{d_k}}\\right)\\mathbf{V}$$\n",
        "\n",
        "We will first calculate the attention score for input $x_1$ for *Steps 6 and 7*. Note that we calculate **one attention value per word vector**. When we reach *Step 8*, we will generalize the attention calculation to the other two input vectors.\n",
        "\n",
        "To obtain $Attention(\\mathbf{Q, K,V})$ for $x_1$, we multiply the intermediate attention score by the 3 value vectors one by one to zoom down into the inner workings of the equation:"
      ],
      "metadata": {
        "id": "5_TvuAJs-uEZ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4Es7A7NOvjD",
        "outputId": "88a8f534-f0d0-4bde-d196-ce568eb75413",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(Fore.LIGHT_RED + Style.BOLD + \"V1: \\n\" + Style.RESET, V[0])\n",
        "print(Fore.LIGHT_RED + Style.BOLD + \"V2: \\n\" + Style.RESET, V[1])\n",
        "print(Fore.LIGHT_RED + Style.BOLD + \"V3: \\n\" + Style.RESET, V[2])\n",
        "print()\n",
        "\n",
        "print(Fore.YELLOW + Style.BOLD + \"Attention 1: \" + Style.RESET)\n",
        "attention1=attention_scores[0].reshape(-1,1)\n",
        "attention1=attention_scores[0][0]*V[0]\n",
        "print(attention1)\n",
        "\n",
        "print(Fore.YELLOW + Style.BOLD + \"Attention 2: \" + Style.RESET)\n",
        "attention2=attention_scores[0][1]*V[1]\n",
        "print(attention2)\n",
        "\n",
        "print(Fore.YELLOW + Style.BOLD + \"Attention 3: \" + Style.RESET)\n",
        "attention3=attention_scores[0][2]*V[2]\n",
        "print(attention3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;9m\u001b[1mV1: \n",
            "\u001b[0m [1. 2. 3.]\n",
            "\u001b[38;5;9m\u001b[1mV2: \n",
            "\u001b[0m [2. 8. 0.]\n",
            "\u001b[38;5;9m\u001b[1mV3: \n",
            "\u001b[0m [2. 6. 3.]\n",
            "\n",
            "\u001b[38;5;3m\u001b[1mAttention 1: \u001b[0m\n",
            "[0.06337894 0.12675788 0.19013681]\n",
            "\u001b[38;5;3m\u001b[1mAttention 2: \u001b[0m\n",
            "[0.93662106 3.74648425 0.        ]\n",
            "\u001b[38;5;3m\u001b[1mAttention 3: \u001b[0m\n",
            "[0.93662106 2.80986319 1.40493159]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Step 6* is complete: The 3 attention values for $x_1$ for each input have been calculated:"
      ],
      "metadata": {
        "id": "4JkJSyGDdxSL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br><center><img src=\"https://drive.google.com/uc?id=1DrE82eezWWpQ4o5OtNPSI8jwFdR6c5u8\" width=\"60%\"></center><br>"
      ],
      "metadata": {
        "id": "zWVFvwC1d1EE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color='#2B4865'>**Step 7: Summing up the results**\n",
        "---\n",
        "</font>"
      ],
      "metadata": {
        "id": "LnJNlhZkB2C-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this step, we sum up the results of *Step 6* to create the first line of the output matrix. The second line will be for the output of the next input, that is, $x_2$ for this example."
      ],
      "metadata": {
        "id": "x8QA1-hWB7Rk"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBDKhaCvOzXj",
        "outputId": "a83637b2-5acf-42a4-bb8d-fa983a369fa0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "attention_input1=attention1+attention2+attention3\n",
        "print(Fore.YELLOW + Style.BOLD + \"Sum Attention for x1: \" + Style.RESET, attention_input1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;3m\u001b[1mSum Attention for x1: \u001b[0m [1.93662106 6.68310531 1.59506841]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see the summed attention value for $x_1$ in the figure below, thus completing the steps for the first input."
      ],
      "metadata": {
        "id": "50Zk51qFfOVI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br><center><img src=\"https://drive.google.com/uc?id=1Na8BqeyuZYNY6E98-3qWiJ3M4QGG2ZCg\" width=\"60%\"></center><br>"
      ],
      "metadata": {
        "id": "dHWvMn5ifL7f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color='#2B4865'>**Step 8: Steps 1 to 7 for all the inputs**\n",
        "---\n",
        "</font>"
      ],
      "metadata": {
        "id": "WNdBH7JWDTQj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Transformer can now produce the attention values of inputs two and three using the same method described in *Steps 1 to 7* for one attention head.\n",
        "\n",
        "From this step onwards, we will assume we have 3 attention values with learned weights with $d_{model}/8=64$. We now want to see what the original dimensions look like when they reach the sublayer's output.\n",
        "\n",
        "We have seen the attention representation process in detail with a small model, so let's now assume we have already generated the 3 attention representations with a dimension of $d_{model}/8=64$:"
      ],
      "metadata": {
        "id": "jyOJEpt9DdOy"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEjgRcqHO4ik",
        "outputId": "51caec3f-f1b1-4216-eed8-51da405c36cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# We assume we have 3 results with learned weights (they were not trained in this example)\n",
        "# We assume we are implementing the original Transformer paper. We will have 3 results of 64 dimensions each\n",
        "attention_head1=np.random.random((3, 64))\n",
        "print(attention_head1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.38464587 0.97440416 0.71904677 0.33169658 0.6099698  0.33166482\n",
            "  0.89708217 0.51369812 0.80581456 0.39938642 0.82353179 0.85588956\n",
            "  0.97686275 0.7680037  0.782911   0.93144075 0.55540618 0.25077953\n",
            "  0.30564976 0.27504848 0.31693901 0.83645785 0.94001682 0.72108591\n",
            "  0.03548944 0.66078724 0.89055292 0.70954996 0.83929716 0.63514769\n",
            "  0.92006666 0.53657751 0.18989152 0.65710533 0.71292925 0.24049143\n",
            "  0.12963479 0.4124736  0.28287217 0.32416978 0.115054   0.0779267\n",
            "  0.88731908 0.18191865 0.25955368 0.10311267 0.49050582 0.78101243\n",
            "  0.18736321 0.59980406 0.30262333 0.60651996 0.89307057 0.98256314\n",
            "  0.95473747 0.4286933  0.06936093 0.52227428 0.64624981 0.27355311\n",
            "  0.19688582 0.51638073 0.06039197 0.65467468]\n",
            " [0.48029979 0.87775841 0.08946413 0.27512154 0.26891502 0.32198958\n",
            "  0.6145797  0.14588073 0.18634514 0.4750353  0.12467958 0.04376944\n",
            "  0.80676552 0.79888299 0.96257485 0.063964   0.29551825 0.75819194\n",
            "  0.05427341 0.38685383 0.3692209  0.49537121 0.62436386 0.45598848\n",
            "  0.69737583 0.61013841 0.45829856 0.04607499 0.62464353 0.17839909\n",
            "  0.54942665 0.7969771  0.10989826 0.28623887 0.03819121 0.17803636\n",
            "  0.82302672 0.91148158 0.10994011 0.9944193  0.70583968 0.00508294\n",
            "  0.60816756 0.41444211 0.71222717 0.66506304 0.13740504 0.82792074\n",
            "  0.14217493 0.12166557 0.65361902 0.21921764 0.09120953 0.38064466\n",
            "  0.92959949 0.60007681 0.14441097 0.10637383 0.42721669 0.75491226\n",
            "  0.07186015 0.41879246 0.59646763 0.70090709]\n",
            " [0.64747567 0.83145534 0.27743639 0.24569509 0.72156483 0.76702927\n",
            "  0.57109523 0.36110509 0.23657961 0.03444951 0.58249803 0.27703104\n",
            "  0.44234131 0.7868717  0.24783489 0.50186834 0.93418811 0.47119599\n",
            "  0.05975231 0.78234748 0.24587485 0.42769963 0.35039607 0.11555908\n",
            "  0.65311475 0.29720924 0.33081241 0.52079157 0.45792456 0.10797859\n",
            "  0.55344565 0.25143124 0.48574902 0.73474295 0.59090895 0.40355451\n",
            "  0.09003677 0.5528826  0.91724605 0.09928403 0.38865816 0.96848121\n",
            "  0.82339868 0.56080317 0.37126674 0.102565   0.79484159 0.82572477\n",
            "  0.31805495 0.45117171 0.75865497 0.05229988 0.34083494 0.07969963\n",
            "  0.02894182 0.7958616  0.60790227 0.58367984 0.68177423 0.59718265\n",
            "  0.84624787 0.12324618 0.19603525 0.31372424]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The result above displayed simulates $z_0$, that is, the 3 output vectors of $d_{model}=64$ for head 1. With this, the Transformer now has the output vectors for the inputs of one head. The next step is to generate the output of the 8 heads to create the final output attention sublayer."
      ],
      "metadata": {
        "id": "W77PBZMaS8Kc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color='#2B4865'>**Step 9: The output of the heads of the attention sublayer**\n",
        "---\n",
        "</font>"
      ],
      "metadata": {
        "id": "qlSQi28hDf4V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this step, we assume that we have trained the 8 heads of the attention sublayer. The Transformer model now has 3 output vectors (of 3 input vectors that are words or word pieces) of $d_{model}=64$ dimensions each:"
      ],
      "metadata": {
        "id": "dkR5pK20UzLp"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QI50dkZ1O630",
        "outputId": "f7cd4779-724d-4a94-bfc0-9fdaf6c005b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "z0h1=np.random.random((3, 64))\n",
        "z1h2=np.random.random((3, 64))\n",
        "z2h3=np.random.random((3, 64))\n",
        "z3h4=np.random.random((3, 64))\n",
        "z4h5=np.random.random((3, 64))\n",
        "z5h6=np.random.random((3, 64))\n",
        "z6h7=np.random.random((3, 64))\n",
        "z7h8=np.random.random((3, 64))\n",
        "print(\"Shape of each head: \", z0h1.shape, \"\\nDimension of 8 heads: \",64*8)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of each head:  (3, 64) \n",
            "Dimension of 8 heads:  512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The 8 heads have now produced $Z$:\n",
        "\n",
        "$$Z = (Z_0,Z_1,Z_2,Z_3,Z_4,Z_5,Z_6,Z_7)$$\n",
        "\n",
        "The Transformer will now concatenate the 8 elements of $Z$ for the final output of the Multi-Head attention sublayer."
      ],
      "metadata": {
        "id": "hNJ_hOBxVk0I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color='#2B4865'>**Step 10: Concatenation of the output of the heads**\n",
        "---\n",
        "</font>"
      ],
      "metadata": {
        "id": "-lDRLbVpDk5M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Transformer concatenates the 8 elements of $Z$:\n",
        "\n",
        "$$MultiHead(Output)= Concat(Z_0,Z_1,Z_2,Z_3,Z_4,Z_5,Z_6,Z_7)W^0 = z, d_{model}$$\n",
        "\n",
        "Note that $Z$ is multiplied by $W^0$, which is a weight matrix that is trained as well. In this model, we will assume $W^0$ is trained and integrated into the concatenation function."
      ],
      "metadata": {
        "id": "Gt54UUvkV795"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3n87LE92_Puf",
        "outputId": "2453c53e-6dae-4ea1-adf7-1186089d43ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "output_attention=np.hstack((z0h1,z1h2,z2h3,z3h4,z4h5,z5h6,z6h7,z7h8))\n",
        "print(Fore.YELLOW + Style.BOLD + \"MultiHead Attention Output :\\n \" + Style.RESET, output_attention)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;3m\u001b[1mMultiHead Attention Output :\n",
            " \u001b[0m [[0.02921319 0.99847071 0.87075338 ... 0.58217167 0.53745708 0.64086444]\n",
            " [0.42067198 0.98503287 0.1770938  ... 0.25072938 0.79252486 0.17376745]\n",
            " [0.54808903 0.90267648 0.90987587 ... 0.57604564 0.54924641 0.62331917]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The concatenation can be visualized as stacking the elements of $Z$ side by side:"
      ],
      "metadata": {
        "id": "4NAJAB1efi1g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br><center><img src=\"https://drive.google.com/uc?id=1e9hXbfE4j3zFgPY_rivRpMdNQsqFAHrQ\" width=\"35%\"></center><br>\n",
        "\n",
        "<br><center><img src=\"https://drive.google.com/uc?id=1D3dyTN3msoLjgt-YoCMkcAyDPfq_h_td\" width=\"60%\"></center><br>"
      ],
      "metadata": {
        "id": "OlRcFeKffm8F"
      }
    }
  ]
}